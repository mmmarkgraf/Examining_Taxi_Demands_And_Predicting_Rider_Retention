{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json, datetime\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was used to tackle a stimulation of technical problems that could be seen in a job interview process.<br><br>\n",
    "The topic of this stimulation was taxi drivers and riders, mainly covering data wrangling, data manipulation, exploratory data analysis (EDA), machine learning (specifically random forest and neural networks was used), and model analysis.<br><br>\n",
    "To run this script, the files logins.json and ultimate_data_challenge.json must be in the same folder as this script. Then, simply run each cell in order.<br><br>\n",
    "Note: Computation times may be long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading logins.json as a dictionary:\n",
    "with open(\"logins.json\", \"r\") as json_reader:\n",
    "    logins_dict = json.load(json_reader)\n",
    "\n",
    "# Loading ultimate_data_challenge.json as a dictionary:\n",
    "with open(\"ultimate_data_challenge.json\", \"r\") as json_reader:\n",
    "    data_dict = json.load(json_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logins_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(logins_dict['login_time']))\n",
    "print(type(logins_dict['login_time'][0]))\n",
    "print(logins_dict['login_time'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data_dict))\n",
    "print(data_dict[0].keys())\n",
    "print(len(data_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 ‑ Exploratory data analysis:\n",
    "\n",
    "\"The attached logins.json file contains (simulated) timestamps of user logins in a particular\n",
    "geographic location. Aggregate these login counts based on 15 minute time intervals, and visualize and describe the resulting time series of login counts in ways that best characterize the underlying patterns of the demand. Please report/illustrate important features of the demand, such as daily cycles. If there are data quality issues, please report them.\"\n",
    "\n",
    "\n",
    "Looking at the loaded login.json file, the the data was stored/loaded in as a dictionary with only one key (login_time) having a list of 93142 elements as the value. The values are dates that were loaded in as string data type.\n",
    "\n",
    "To answer the problem, I want to visualize the demands each 15 minutes by weekdays. To do this, the following procedure will be performed:\n",
    "- Converting the dictionary into a list/extracting the single list from the dictionary\n",
    "- Convert the values into a time data type\n",
    "- Count the number of demands per 15 minutes\n",
    "- Assign weekday to each 15 minute interval\n",
    "- Graph the demands:\n",
    "    - Daily\n",
    "    - Weekly\n",
    "    - Monthly\n",
    "    \n",
    "One thing to note about the data in logins.json and how the script was written:\n",
    "- The dates range from 1-1-1970 to 4-13-1970 (checked using the max() function)\n",
    "- 15 minute intervals start at midnight of 1-1-1970 (at 0 hours: 0 minutes: 0 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the single value of the dictionary, logins_dict:\n",
    "logins_list = logins_dict['login_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the values into datetime objects:\n",
    "logins_list_converted = [datetime.strptime(date, '%Y-%m-%d %H:%M:%S') for date in logins_list]\n",
    "\n",
    "# Checking the last/latest date in the list:\n",
    "print(max(logins_list_converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame to count the number of timestamps within 15 minutue intervals:\n",
    "start_datetime = datetime(year= 1970, month= 1, day= 1)\n",
    "\n",
    "## The end_datetime was determined by rounding up the last data entry to the next 15 minute interval (1970-04-13 18:57:38):\n",
    "end_datetime = datetime(year= 1970, month= 4, day= 13, hour= 19)\n",
    "\n",
    "time_interval = timedelta(minutes= 15)\n",
    "\n",
    "changing_datetime = start_datetime\n",
    "\n",
    "interval_list = []\n",
    "\n",
    "while changing_datetime >= start_datetime and changing_datetime < end_datetime:\n",
    "    interval_list.append(changing_datetime)\n",
    "    changing_datetime = changing_datetime + time_interval\n",
    "\n",
    "login_df_interval = pd.DataFrame({'TIMESTAMP_START_INTERVAL': interval_list, \n",
    "                                 'COUNT': np.NaN, \n",
    "                                 'WEEK': np.NaN, \n",
    "                                 'WEEKDAY': np.NaN, \n",
    "                                 'MONTH': np.NaN})\n",
    "\n",
    "for index_interval, date_interval in login_df_interval.iterrows():\n",
    "    start_time = date_interval.TIMESTAMP_START_INTERVAL\n",
    "    end_time = date_interval.TIMESTAMP_START_INTERVAL + time_interval\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    ## The TIMESTAMP_START_INTERVAL refers to the start time to the second before the next TIMESTAMP_START_INTERVAL:\n",
    "    for time_logins in logins_list_converted:\n",
    "        if time_logins >= start_time and time_logins < end_time:\n",
    "            counter += 1\n",
    "    \n",
    "    login_df_interval.loc[index_interval, 'COUNT'] = counter\n",
    "    \n",
    "    ## Adding week number, weekday, and month to the DataFrame:\n",
    "    date_tuple = tuple()\n",
    "    week_number = np.NaN\n",
    "    weekday_int = np.NaN\n",
    "    weekday_str = str()\n",
    "    month = np.NaN\n",
    "    \n",
    "    ### .isocalendar() returns a tuple, (year, week number, weekday), starting at 1/\"iso\" [not 0]:\n",
    "    date_tuple = date_interval.TIMESTAMP_START_INTERVAL.isocalendar()\n",
    "    \n",
    "    week_number = date_tuple[1]\n",
    "    \n",
    "    weekday_int = date_tuple[2]\n",
    "    \n",
    "    month = date_interval.TIMESTAMP_START_INTERVAL.date().month\n",
    "    \n",
    "    ### Assigning string weekday to each date:\n",
    "    if weekday_int == 1:\n",
    "        weekday_str = 'Monday'\n",
    "        \n",
    "    elif weekday_int == 2:\n",
    "        weekday_str = 'Tuesday'\n",
    "        \n",
    "    elif weekday_int == 3:\n",
    "        weekday_str = 'Wednesday'\n",
    "        \n",
    "    elif weekday_int == 4:\n",
    "        weekday_str = 'Thursday'\n",
    "        \n",
    "    elif weekday_int == 5:\n",
    "        weekday_str = 'Friday'\n",
    "        \n",
    "    elif weekday_int == 6:\n",
    "        weekday_str = 'Saturday'\n",
    "        \n",
    "    elif weekday_int == 7:\n",
    "        weekday_str = 'Sunday'\n",
    "        \n",
    "    else:\n",
    "        weekday_str = np.NaN\n",
    "    \n",
    "    login_df_interval.loc[index_interval, 'WEEK'] = week_number\n",
    "    login_df_interval.loc[index_interval, 'WEEKDAY'] = weekday_str\n",
    "    login_df_interval.loc[index_interval, 'MONTH'] = month\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis Begins: -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## Single Date #############################################################\n",
    "\n",
    "## Graphing the date with the most count in a 15 minute period:\n",
    "max_count_date = pd.Timestamp(login_df_interval.TIMESTAMP_START_INTERVAL[login_df_interval.COUNT == max(login_df_interval.COUNT)].values[0]).date()\n",
    "\n",
    "max_count_date_df = pd.DataFrame(columns=login_df_interval.columns)\n",
    "\n",
    "for index, row in login_df_interval.iterrows():\n",
    "    date = row.TIMESTAMP_START_INTERVAL.date()\n",
    "\n",
    "    \n",
    "    if date == max_count_date:\n",
    "        temp_df = pd.DataFrame({'TIMESTAMP_START_INTERVAL': row.TIMESTAMP_START_INTERVAL, \n",
    "                        'COUNT': row.COUNT, \n",
    "                        'WEEK': row.WEEK, \n",
    "                        'WEEKDAY': row.WEEKDAY, \n",
    "                        'MONTH': row.MONTH}, index=[0])\n",
    "            \n",
    "        max_count_date_df = pd.concat([max_count_date_df, temp_df], keys= row.index, ignore_index= True)\n",
    "\n",
    "plt.figure(figsize=(20, 6))        \n",
    "plt.plot(max_count_date_df.TIMESTAMP_START_INTERVAL, max_count_date_df.COUNT)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Time (Month-Day Hour)')\n",
    "plt.ylabel('Demand Per 15 Minute Interval')\n",
    "plt.title('Most Count in a Single 15 Minute Interval, 3-1-1970')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Daily Comparison ###########################################################\n",
    "# Comparing by weekday, Cumulative:\n",
    "weekday_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "#### This list will be used in the next cell for weekly comparison:\n",
    "total_week_sum = []\n",
    "\n",
    "for weekday in weekday_list:\n",
    "    weekday_df = pd.DataFrame(columns=['TIMESTAMP_START_INTERVAL', 'COUNT','NUMBER_OF_OCCURANCES','AVERAGE'])\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_list = []\n",
    "    \n",
    "    for index, row in login_df_interval.iterrows():\n",
    "        if weekday == row.WEEKDAY:\n",
    "            interval = row.TIMESTAMP_START_INTERVAL.time().isoformat()\n",
    "            \n",
    "            if interval in list(weekday_df.TIMESTAMP_START_INTERVAL):\n",
    "                weekday_df.loc[weekday_df.TIMESTAMP_START_INTERVAL == interval, 'COUNT'] = weekday_df.loc[weekday_df.TIMESTAMP_START_INTERVAL == interval].COUNT.values[0] + row.COUNT\n",
    "                weekday_df.loc[weekday_df.TIMESTAMP_START_INTERVAL == interval, 'NUMBER_OF_OCCURANCES'] = weekday_df.loc[weekday_df.TIMESTAMP_START_INTERVAL == interval].NUMBER_OF_OCCURANCES.values[0] + row.COUNT\n",
    "                \n",
    "                average = weekday_df.COUNT[weekday_df.TIMESTAMP_START_INTERVAL == interval].values[0] / weekday_df.NUMBER_OF_OCCURANCES[weekday_df.TIMESTAMP_START_INTERVAL == interval].values[0]\n",
    "                weekday_df.loc[weekday_df.TIMESTAMP_START_INTERVAL == interval, 'AVERAGE'] = average\n",
    "            \n",
    "            else:\n",
    "                temp_df = pd.DataFrame({'TIMESTAMP_START_INTERVAL': interval, \n",
    "                                        'COUNT': row.COUNT,\n",
    "                                        'NUMBER_OF_OCCURANCES': 1,\n",
    "                                        'AVERAGE': row.COUNT}, \n",
    "                                       index=[0])\n",
    "\n",
    "                weekday_df = pd.concat([weekday_df, temp_df], ignore_index= True)\n",
    "            \n",
    "            if row.WEEK != 1 and row.WEEK != 16:\n",
    "                temp_list.append(row.COUNT)\n",
    "    \n",
    "    \n",
    "    total_week_sum.append(sum(temp_list))\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))        \n",
    "    plt.plot(weekday_df.TIMESTAMP_START_INTERVAL, weekday_df.COUNT)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylim(0, 700)\n",
    "    plt.xlabel('Time (Month-Day Hour)')\n",
    "    plt.ylabel('Demand Per 15 Minute Interval (Cumulative)')\n",
    "    plt.title('All ' + weekday + 's')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Weekly Comparison ###########################################################\n",
    "# Comparing each week:\n",
    "week_number_range = range(1, int(max(login_df_interval.WEEK)) + 1)\n",
    "\n",
    "week_counts = []\n",
    "\n",
    "for week_number in week_number_range:\n",
    "    temp_list = []\n",
    "    \n",
    "    for index, row in login_df_interval.iterrows():\n",
    "        if week_number == row.WEEK:\n",
    "            temp_list.append(row.COUNT)\n",
    "    \n",
    "    week_counts.append(sum(temp_list))\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20, 6))        \n",
    "plt.plot(week_number_range, week_counts)\n",
    "plt.xticks(week_number_range, rotation=90)\n",
    "plt.hlines(week_counts[0]/3*7, xmin= 1, xmax= 2) ## This is the daily average for the first week, multiplied by 7 days\n",
    "plt.hlines(week_counts[-1]/2*7, xmin= 15, xmax= 16) ## This is the daily average for the last week, multiplied by 7 days\n",
    "plt.xlabel('Week Number (For Year 1970)')\n",
    "plt.ylabel('Demand (Cumulative)')\n",
    "plt.title('Weeks (January-April, 1970)')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Monthly Comparison #########################################################\n",
    "# Comparing each month:\n",
    "month_number_range = range(1, int(max(login_df_interval.MONTH)) + 1)\n",
    "\n",
    "month_counts = []\n",
    "\n",
    "for month_number in month_number_range:\n",
    "    temp_list = []\n",
    "    \n",
    "    for index, row in login_df_interval.iterrows():\n",
    "        if month_number == row.MONTH:\n",
    "            temp_list.append(row.COUNT)\n",
    "    \n",
    "    month_counts.append(sum(temp_list))\n",
    "    \n",
    "plt.figure(figsize=(20, 6))        \n",
    "plt.plot(month_number_range, month_counts)\n",
    "plt.xticks(month_number_range, rotation=90)\n",
    "plt.hlines(month_counts[-1]/13*30, xmin= 3, xmax= 4) ## This is the daily average of April, multiplied by 30 days\n",
    "plt.xlabel('Month (Numeric)')\n",
    "plt.ylabel('Demand (Cumulative)')\n",
    "plt.title('Demand During January-April, 1970')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the patterns of demand in the various graphs generated, here is a summary of some of the patterns seen:\n",
    "- Daily Graphs:\n",
    "    - Monday:\n",
    "\t\t- High Demand:\n",
    "\t\t\t- The highest trend of the day is typically at midday, with the trend starting around 8:30 AM, peaking at 11:00 AM - 11:45 AM, and then demand decreases until 1:30 PM\n",
    "\t\t\t- There is also significant uptrends that begins at 7:00 PM, peaking and plateauing from 8:45 PM to the next day (Tuesday) 1:00 AM\n",
    "\t\t\t\t- This uptrend peaks and plateaus about half of the max demand during midday \n",
    "\t\t\t- Demand from Sunday night also carries over into the early morning of Monday and begins decreasing after 12:30 AM\n",
    "\t\t\t\t- The demand is also half of the max demand of Monday midday\n",
    "\t\t- Low Demand:\n",
    "\t\t\t- The lowest ranges are during the morning hours between 6:00 AM to the start of the highest trend at 8:30 AM\n",
    "\t\t\t\t- The lowest demand is between 6:15 AM - 6:30 AM, which is significantly lower then rest of the day (seems to almost be in the single digits of counts even though the graph shows the cumulative demand of all Mondays)\n",
    "\t\t\t- There is also another low range after the highest trend ends at 1:30 PM and lastest until the start of the second uptrend at 7:00 PM\n",
    "\n",
    "\t- Tuesday - Thursday:\n",
    "\t\t- Tuesday - Thursday follow the same pattern as Monday except that the demand in the night hours (including the hours from midnight to 6:00 AM) increases as the days progress through the week\n",
    "\t\t\t- On Thursday, the high demand from 10:30 PM - 11:00 PM is almost the same demand as seen during the midday peak \n",
    "\t\t\t\n",
    "\t- Friday:\n",
    "\t\t- Similar to the Thursday, Friday has a significant increase to the demands during night hours, which peaks about 1.5 times more than the midday demand.\n",
    "\t\t\n",
    "\t- Saturday:\n",
    "\t\t- Saturday's pattern carries over from Friday and peaks at 4:45 A.M.\n",
    "\t\t- A low demand range follows after until 10:00 AM, where the demand begins to uptrend and plateaus at 1:00 PM until 8:00 PM, which begins a new uptrend that continues into Sunday\n",
    "\t\t\n",
    "\t- Sunday:\n",
    "\t\t- Sunday is similar to Saturday's pattern except it has the highest demand peak of the weak between 4:30 AM - 5:00 AM\n",
    "\t\t- However, the night hours of Sunday does not uptrend and continues the plateau that begins from midday and lasts into Monday 1:00 AM\n",
    "\t\t\n",
    "\t- Note:\n",
    "\t\t- Because this is a cumluative of all counts and there are not the equal amounts of each day of week, some days will have less or higher values. However, since there are at most 15 Sundays, Mondays, Thursdays, Fridays, and Saturdays, and at least 14 Tuesdays and Wednesdays, there should not be a significant difference in the aggregation of counts. This could be seen in the Tuesday and Wednesday daily graphs as the demand can still be seen to increase in the night hours comparatively to Monday's daily graph.\n",
    "\t\t\n",
    "- Weekly Graphs:\n",
    "\t- The demand seem to increase as the weeks progress\n",
    "\t- Note:\n",
    "\t\t- Because Week 1 only contains Thursday - Saturday and Week 16 only contains Sunday and Monday, these two weeks are not good to use as comparisons to the other weeks\n",
    "\t\t\t- Two black horizontal lines were added to extrapolate/simulate the rest of the week's demand\n",
    "\t\t\t\t- However, this extrapolation was done with simple averaging and then multiplying 7 days--and as shown on the graphs, significantly undershoots or overshoots\n",
    "\t\t\t\t\t- This could be aproached using various methods (such as filling in the missing dates with random forest or filling the missing days with daily averages)--however, since there isn't enough data for better weekly and monthly analysis, these methods could also undershoot and overshoot just as dipicted by the two horizontal lines.\n",
    "\n",
    "- Monthly Graphs:\n",
    "\t- With the data provided, the demand seems to generally increase as the months progressed\n",
    "\t- The horizontal was to extrapolate for April (Month 4) by dividing the total counts in April by 13 days and then multiplying by 30 days.\n",
    "\n",
    "- Note:\n",
    "\t- All graphs, the last data entry in the dataset is on 4-13-1970 at 18:54:23, meaning that there was not enough data to complete the entire day of 4-13-1970\n",
    "    \n",
    "In summary:<br>\n",
    "    For Monday - Friday, the demand peaks at midday and in the night hours. As the week progresses, the demand in the night hours begin to increase to the point where on Fridays, the night hour peaks are larger than then the midday peak. The graphs for Monday - Friday resemebles a \"W\"-shape. <br>\n",
    "    For Saturday and Sunday, the demands are highest at the early morning hours (before 5 AM) and then begins a second uptrend after 1 PM. The graphs for Saturday and Sunday resemebles a \"U\"-shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 ‑ Experiment and metrics design\n",
    "The neighboring cities of Gotham and Metropolis have complementary circadian rhythms: on\n",
    "weekdays, Ultimate Gotham is most active at night, and Ultimate Metropolis is most active\n",
    "during the day. On weekends, there is reasonable activity in both cities.\n",
    "However, a toll bridge, with a two way\n",
    "toll, between the two cities causes driver partners to tend\n",
    "to be exclusive to each city. The Ultimate managers of city operations for the two cities have\n",
    "proposed an experiment to encourage driver partners to be available in both cities, by\n",
    "reimbursing all toll costs.\n",
    "1. What would you choose as the key measure of success of this experiment in\n",
    "encouraging driver partners to serve both cities, and why would you choose this metric?\n",
    "2. Describe a practical experiment you would design to compare the effectiveness of the\n",
    "proposed change in relation to the key measure of success. Please provide details on:\n",
    "a. how you will implement the experiment\n",
    "b. what statistical test(s) you will conduct to verify the significance of the\n",
    "observation\n",
    "c. how you would interpret the results and provide recommendations to the city\n",
    "operations team along with any caveats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "I would choose the traffic of driver partners that uses the toll bridge before and after the reimbursement experiment as the main metric. If there are other longer, less direct routes between the cities, the toll bridge could be faster and more cost-efficient to take after reimbursement, which increases traffic on the toll bridge. If the toll bridge is the only route, then there should be a increase of driver partners who are more willing travel across the bridge without having to factor in the tolls. If possible, examining the driver partners who rarely use the toll bridge before reimbursement would be the main determinants of success as this seems to be the targets of the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:<br>\n",
    "A) The data to record before the policy is in effect (or announced) is the amount of times each driver partner pass through the toll bridge, as well as the direction of the travel. Then, after a month, put the policy into effect and record the same data for another two months. For the three months, it would be better to choose months with similar consecutive traffic patterns to minimize the effect of other variables. A example is experimenting according to summer months after June as students are in summer vacation instead of May-July where students are in school at the beginning and out of school at the end of the period. Unless there is already previous data avaliable (at least one year), then simply put execute the policy and record the data. Another metric to record is the amount of reimbursement being allocated to the driver partners and amount of tolls collected from the toll bridge.<br><br>\n",
    "B) Depending on the results and length of the data, it would be better to ignore the first few months (if possible) after the policy is in place as driver partners could still be adjusting to the change--expecting a plateau of driver partners using the toll bridge after a time has passed. The type of statistics to use depends of the results of the data. If the traffic has dramatically increased for driver partners who were labelled as city-exclusive, then simple statistics should be sufficient. If the traffic has not largely increased or did not increase at all for city-exclusive driver partners, then obtaining a p-value could be a measurement of success. In all cases, examining the difference between the tolls paid by driver partners before the policy and the total reimbursement amount would show the complete overview of traffic by all driver partners.<br><br>\n",
    "C) Depending on the goals of the city operations team, if the traffic has largely increased for city-exclusive driver partners and there is a large difference in reimbursement vs. tolls collected from driver partners (more reimbursement than tolls collected), then I would say that the policy does increase traffic between each city. If the difference is small (or even if a p-value is needed to determine significance), I would say that the policy might be statisitically significant but does not have a large effect in the overall picture of making driver partners non-exclusive to a specific city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 ‑ Predictive modeling\n",
    "Ultimate is interested in predicting rider retention. To help explore this question, we have\n",
    "provided a sample dataset of a cohort of users who signed up for an Ultimate account in\n",
    "January 2014. The data was pulled several months later; we consider a user retained if they\n",
    "were “active” (i.e. took a trip) in the preceding 30 days.\n",
    "We would like you to use this data set to help understand what factors are the best predictors\n",
    "for retention, and offer suggestions to operationalize those insights to help Ultimate.\n",
    "The data is in the attached file ultimate_data_challenge.json. See below for a detailed\n",
    "description of the dataset. Please include any code you wrote for the analysis and delete the\n",
    "dataset when you have finished with the challenge.\n",
    "1. Perform any cleaning, exploratory analysis, and/or visualizations to use the provided\n",
    "data for this analysis (a few sentences/plots describing your approach will suffice). What\n",
    "fraction of the observed users were retained?\n",
    "2. Build a predictive model to help Ultimate determine whether or not a user will be active\n",
    "in their 6th month on the system. Discuss why you chose your approach, what\n",
    "alternatives you considered, and any concerns you have. How valid is your model?\n",
    "Include any key indicators of model performance.\n",
    "3. Briefly discuss how Ultimate might leverage the insights gained from the model to\n",
    "improve its longterm\n",
    "rider retention (again, a few sentences will suffice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 - Data Cleaning:\n",
    "## The JSON file has already been loading in the first few cells.\n",
    "\n",
    "# Creating a DataFrame from the JSON file loaded in as a list of dictionaries, data_dict:\n",
    "rider_df = pd.DataFrame(columns= list(data_dict[0].keys()).append('active'))\n",
    "\n",
    "date_difference = timedelta(days= 30)\n",
    "\n",
    "current_date = datetime.strptime('2014-07-01', '%Y-%m-%d') # This is the latest date in the JSON file\n",
    "\n",
    "for data in data_dict:\n",
    "    temp_df = pd.DataFrame(data, index=[0])\n",
    "    \n",
    "    active_status = np.NaN\n",
    "    last_date = datetime.strptime(data['last_trip_date'], '%Y-%m-%d')\n",
    "    \n",
    "    if last_date + date_difference >= current_date:\n",
    "        active_status = 'Active'\n",
    "        \n",
    "    else:\n",
    "        active_status = 'Inactive'\n",
    "    \n",
    "    temp_df['active'] = active_status\n",
    "\n",
    "    rider_df = pd.concat([rider_df, temp_df], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rider_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 - Exploratory Data Analysis:\n",
    "\n",
    "## Want to count the cities of users:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(rider_df.city[rider_df.active == 'Active'])\n",
    "plt.ylim(0, 16000)\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of Active Users')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rider_df.city[rider_df.active == 'Inactive'], color= 'red')\n",
    "plt.ylim(0, 16000)\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of Inactive Users')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to see the Operating System/Device:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(rider_df.phone[rider_df.active == 'Active'].fillna('NA'))\n",
    "plt.ylim(0, 20000)\n",
    "plt.xlabel('Operating System')\n",
    "plt.ylabel('Number of Active Users')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rider_df.phone[rider_df.active == 'Inactive'].fillna('NA'), color= 'red')\n",
    "plt.ylim(0, 20000)\n",
    "plt.xlabel('Operating System')\n",
    "plt.ylabel('Number of Inactive Users')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to see the User's Rating (by driver) of \"active users\":\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(rider_df.avg_rating_by_driver[rider_df.active == 'Active'])\n",
    "plt.ylim(0, 30000)\n",
    "plt.xlabel('Average Rating (by Drivers)')\n",
    "plt.ylabel('Number of Active Users')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rider_df.avg_rating_by_driver[rider_df.active == 'Inactive'], color = 'red')\n",
    "plt.ylim(0, 30000)\n",
    "plt.xlabel('Average Rating (by Drivers)')\n",
    "plt.ylabel('Number of Inactive Users')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to see the average distance of trips:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(rider_df.avg_dist[rider_df.active == 'Active'], bins=60)\n",
    "plt.ylim(0, 10000)\n",
    "plt.xlim(0, 60)\n",
    "plt.xlabel('Average Distance of Trips')\n",
    "plt.ylabel('Number of Active Users')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rider_df.avg_dist[rider_df.active == 'Inactive'], color = 'red', bins=60)\n",
    "plt.ylim(0, 10000)\n",
    "plt.xlim(0, 60)\n",
    "plt.xlabel('Average Distance of Trips')\n",
    "plt.ylabel('Number of Inactive Users')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to see the percentage of the total trips made during a weekday:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(rider_df.weekday_pct[rider_df.active == 'Active'], bins=60)\n",
    "plt.ylim(0, 8000)\n",
    "plt.xlim(0, 60)\n",
    "plt.xlabel('Percentage of Trips Made on a Weekday')\n",
    "plt.ylabel('Number of Active Users')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rider_df.weekday_pct[rider_df.active == 'Inactive'], color = 'red', bins=60)\n",
    "plt.ylim(0, 8000)\n",
    "plt.xlim(0, 60)\n",
    "plt.xlabel('Percentage of Trips Made on a Weekday')\n",
    "plt.ylabel('Number of Inactive Users')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_percentage = len(rider_df[rider_df.active == 'Active']) / len(rider_df)\n",
    "print(retention_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 - What fraction of the observed users were retained?:<br><br>\n",
    "Of the fraction of total users, 37.608% of users who join in Janurary 2014 are considered \"active\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 - Predictive Modeling:\n",
    "## For the model I will be using is Random Forest because it is complex enough to deal with the number of variables.\n",
    "## Also, because random forest was used, cross-validation is not necessary as the variable shuffling and dataset will be \n",
    "## shuffled.\n",
    "### A neural network model was included in cells further.\n",
    "## Columns with dates will be omitted.\n",
    "\n",
    "# Changing string values in rider_df to numerical values:\n",
    "rider_df_numeric = pd.DataFrame(columns=list(['city', 'trips_in_first_30_days', 'avg_rating_of_driver','avg_surge', \n",
    "                                              'phone', 'surge_pct','ultimate_black_user', 'weekday_pct', 'avg_dist',\n",
    "                                              'avg_rating_by_driver']))\n",
    "label_list = []\n",
    "\n",
    "for index, user in rider_df.iterrows():\n",
    "    if user['city'] == \"King's Landing\":\n",
    "        rider_df_numeric.loc[index, 'city'] = 0\n",
    "        \n",
    "    elif user['city'] == \"Astapor\":\n",
    "        rider_df_numeric.loc[index, 'city'] = 1\n",
    "        \n",
    "    else:\n",
    "        rider_df_numeric.loc[index, 'city'] = 3\n",
    "        \n",
    "    rider_df_numeric.loc[index, 'trips_in_first_30_days'] = user['trips_in_first_30_days']\n",
    "\n",
    "    if np.isnan(user['avg_rating_of_driver']): ## Replacing np.NaN with average\n",
    "        rider_df_numeric.loc[index, 'avg_rating_of_driver'] = np.nanmean(rider_df.avg_rating_of_driver)\n",
    "    \n",
    "    else:\n",
    "        rider_df_numeric.loc[index, 'avg_rating_of_driver'] = user['avg_rating_of_driver']\n",
    "        \n",
    "    rider_df_numeric.loc[index, 'avg_surge'] = user['avg_surge']\n",
    "    \n",
    "    if user['phone'] == 'iPhone':\n",
    "        rider_df_numeric.loc[index, 'phone'] = 0\n",
    "        \n",
    "    elif user['phone'] == 'Android':\n",
    "        rider_df_numeric.loc[index, 'phone'] = 1\n",
    "    \n",
    "    else:\n",
    "        rider_df_numeric.loc[index, 'phone'] = 0  ## All np.NaN are replaced with iPhone\n",
    "    \n",
    "    rider_df_numeric.loc[index, 'surge_pct'] = user['surge_pct']\n",
    "    rider_df_numeric.loc[index, 'ultimate_black_user'] = int(user['ultimate_black_user'])\n",
    "    rider_df_numeric.loc[index, 'weekday_pct'] = user['weekday_pct']\n",
    "    rider_df_numeric.loc[index, 'avg_dist'] = user['avg_dist']\n",
    "    \n",
    "    if np.isnan(user['avg_rating_by_driver']): ## Replacing np.NaN with average\n",
    "        rider_df_numeric.loc[index, 'avg_rating_by_driver'] = np.nanmean(rider_df.avg_rating_by_driver)\n",
    "        \n",
    "    else:\n",
    "        rider_df_numeric.loc[index, 'avg_rating_by_driver'] = user['avg_rating_by_driver']\n",
    "\n",
    "    if user['active'] == 'Inactive':\n",
    "        label_list.append(0)\n",
    "        \n",
    "    else:\n",
    "        label_list.append(1)\n",
    "                \n",
    "rider_df_numeric = rider_df_numeric.astype(dtype=np.float64) ## Oddly, the data types changed to \"object\"\n",
    "rider_df_label = pd.factorize(rider_df['active'])[0]\n",
    "\n",
    "# Splitting the data into test and train datasets:\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(rider_df_numeric, rider_df_label, test_size= 0.25, shuffle= True)\n",
    "\n",
    "# Optimizing hyperparameter for best number of trees:\n",
    "trees_list = list(range(64, 129))\n",
    "accuracy_score_list = []\n",
    "\n",
    "for trees in trees_list:\n",
    "    random_forest = RandomForestClassifier(n_estimators= trees, max_features= 'sqrt', n_jobs= -1)\n",
    "    random_forest.fit(X_train, Y_train)\n",
    "    accuracy_score_list.append(random_forest.score(X_test, Y_test))\n",
    "    \n",
    "## Selecting the best number of trees\n",
    "best_accuracy_score = max(accuracy_score_list)\n",
    "\n",
    "for element, score in enumerate(accuracy_score_list):\n",
    "    if score == best_accuracy_score:\n",
    "        best_trees = trees_list[element]\n",
    "        \n",
    "random_forest = RandomForestClassifier(n_estimators= best_trees, max_features= 'sqrt', n_jobs= -1)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "predictions = random_forest.predict(X_test)\n",
    "accuracy_score = random_forest.score(X_test, Y_test)\n",
    "\n",
    "## Extracting the weights for each input variable:\n",
    "feature_coefficients = random_forest.feature_importances_\n",
    "\n",
    "temp_dict = {'city': feature_coefficients[0], \n",
    "             'trips_in_first_30_days': feature_coefficients[1], \n",
    "             'avg_rating_of_driver': feature_coefficients[2],\n",
    "             'avg_surge': feature_coefficients[3], \n",
    "             'phone': feature_coefficients[4], \n",
    "             'surge_pct': feature_coefficients[5],\n",
    "             'ultimate_black_user': feature_coefficients[6], \n",
    "             'weekday_pct': feature_coefficients[7], \n",
    "             'avg_dist': feature_coefficients[8],\n",
    "             'avg_rating_by_driver': feature_coefficients[9]}\n",
    "\n",
    "feature_coefficients_df = pd.DataFrame(temp_dict, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model:\n",
    "nn_model = keras.Sequential()\n",
    "nn_model.add(keras.layers.Dense(20, activation='relu', input_shape=(10,)))\n",
    "nn_model.add(keras.layers.Dense(10, activation='relu'))\n",
    "nn_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "nn_model.compile('adam', loss= 'binary_crossentropy', metrics=['accuracy']) # Accuracy can be used since the problem is categorical\n",
    "nn_model.fit(X_train, Y_train, epochs=50, validation_split= 0.20)\n",
    "nn_model_prediction = nn_model.predict(X_test)\n",
    "nn_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score)\n",
    "print(best_trees)\n",
    "feature_coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Random Forest Metrics:\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Neural Network Metrics:\n",
    "nn_predictions_convert = []\n",
    "\n",
    "for predict in nn_model_prediction:\n",
    "    nn_predictions_convert.append(round(predict[0]))\n",
    "    \n",
    "print(classification_report(Y_test, nn_predictions_convert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 - Model Analysis:<br><br>\n",
    "    The model using basic Random Forest performed sufficiently with an accuracy of ~75%. Other model types could of been used, such as kNN, but since the goal of the question is what feature of the user could increase usage, the simple extraction of feature weights/coefficients are sufficient for this purpose. A simple neural network was created and the model had accuracy score of ~76%, which is similar to the performance of the random forest model. However, since the nature of neural networks does not provide a uniform way of determining feature importance, it can only be used for predictions.<br><br>\n",
    "    The three features with highest impact according to the random forest model are the average distance of the trips, trips made during the weekdays, and the average user rating given by the driver. However, when comparing with the histograms, the difference between active and inactive users was not apparent.<br><br>\n",
    "    Because the question is a binary classification problem, the accuracy would be the best statistic to determining the performance of the model. Looking at different statistics for both models, the models were better at predicting active users (1) than inactive users (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:<br><br>\n",
    "    Ultimate Technologies can attempt to market their product according to the features weighted most important by the random forest model. An example could be if many of the trips made during the weekday are from people being driven to and from work, then marketing towards workers would bring more of the same type of customers. Another example could be to create a deal or program for customers who only need to travel a small distance as most customers travel less than 20 miles in one trip. For high rated users who have made enough trips greater than a certain number of rides, these customers could be sent deals to encourage more incentive to choose Ultimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
